{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a gensim\n",
    "\n",
    "Los ejemplos aquí mostrados fueron tomados de https://www.machinelearningplus.com/nlp/gensim-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a Dictionary from a list of sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fhbapto\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.6-win-amd64.egg\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# Show the word to id map\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "\n",
    "# If you check now, the dictionary should have been updated with the new words (tokens).\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a Dictionary from one or more text files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'army': 0, 'china': 1, 'chinese': 2, 'force': 3, 'liberation': 4, 'of': 5, 'people': 6, 'recently': 7, 'recruited': 8, 'rocket': 9, 'tank': 10, 'technicians': 11, 'the': 12, 'think': 13, 'companies': 14, 'daily': 15, 'from': 16, 'on': 17, 'pla': 18, 'private': 19, 'reported': 20, 'saturday': 21, 'and': 22, 'appointment': 23, 'at': 24, 'ceremony': 25, 'experts': 26, 'founding': 27, 'hao': 28, 'letters': 29, 'other': 30, 'received': 31, 'science': 32, 'technology': 33, 'zhang': 34, 'according': 35, 'by': 36, 'defense': 37, 'national': 38, 'panel': 39, 'published': 40, 'report': 41, 'to': 42, 'as': 43, 'fellow': 44, 'his': 45, 'honored': 46, 'will': 47, 'conduct': 48, 'design': 49, 'fields': 50, 'into': 51, 'like': 52, 'members': 53, 'overall': 54, 'research': 55, 'serve': 56, 'which': 57, 'five': 58, 'for': 59, 'launching': 60, 'missile': 61, 'missiles': 62, 'network': 63, 'system': 64, 'years': 65, 'counterparts': 66, 'enjoy': 67, 'firms': 68, 'owned': 69, 'said': 70, 'same': 71, 'state': 72, 'their': 73, 'treatment': 74, 'civilian': 75, 'deepening': 76, 'development': 77, 'in': 78, 'integration': 79, 'marks': 80, 'military': 81, 'new': 82, 'that': 83, 'this': 84, 'better': 85, 'capabilities': 86, 'combat': 87, 'contribute': 88, 'could': 89, 'enhancement': 90, 'innovation': 91, 'make': 92}\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt', encoding='utf-8'))\n",
    "\n",
    "# Token to Id map\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadTxtFiles(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='latin'):\n",
    "                yield simple_preprocess(line)\n",
    "\n",
    "path_to_text_directory = \"lsa_sports_food_docs\"\n",
    "\n",
    "dictionary = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accompanying': 309,\n",
       " 'according': 175,\n",
       " 'achaya': 176,\n",
       " 'across': 0,\n",
       " 'activity': 1,\n",
       " 'ad': 177,\n",
       " 'added': 310,\n",
       " 'advances': 69,\n",
       " 'advantage': 486,\n",
       " 'after': 96,\n",
       " 'ago': 311,\n",
       " 'aid': 386,\n",
       " 'all': 143,\n",
       " 'allow': 487,\n",
       " 'along': 144,\n",
       " 'already': 178,\n",
       " 'also': 207,\n",
       " 'alters': 488,\n",
       " 'although': 2,\n",
       " 'america': 466,\n",
       " 'amongst': 268,\n",
       " 'an': 97,\n",
       " 'ancient': 179,\n",
       " 'and': 3,\n",
       " 'another': 269,\n",
       " 'any': 98,\n",
       " 'are': 4,\n",
       " 'areas': 422,\n",
       " 'around': 70,\n",
       " 'as': 5,\n",
       " 'association': 208,\n",
       " 'associazione': 447,\n",
       " 'at': 99,\n",
       " 'attempts': 71,\n",
       " 'available': 387,\n",
       " 'back': 489,\n",
       " 'badminton': 6,\n",
       " 'baked': 423,\n",
       " 'baking': 357,\n",
       " 'ball': 58,\n",
       " 'baseball': 59,\n",
       " 'bases': 72,\n",
       " 'bat': 60,\n",
       " 'batsmen': 100,\n",
       " 'batter': 73,\n",
       " 'batting': 61,\n",
       " 'be': 7,\n",
       " 'beach': 8,\n",
       " 'beans': 145,\n",
       " 'became': 209,\n",
       " 'because': 210,\n",
       " 'become': 424,\n",
       " 'been': 101,\n",
       " 'between': 62,\n",
       " 'birthplace': 180,\n",
       " 'black': 235,\n",
       " 'body': 236,\n",
       " 'boiling': 312,\n",
       " 'both': 401,\n",
       " 'bounce': 490,\n",
       " 'bounces': 491,\n",
       " 'breakfast': 237,\n",
       " 'breaks': 238,\n",
       " 'broad': 358,\n",
       " 'but': 281,\n",
       " 'by': 9,\n",
       " 'cake': 239,\n",
       " 'cakes': 240,\n",
       " 'called': 102,\n",
       " 'calzone': 467,\n",
       " 'can': 146,\n",
       " 'casual': 10,\n",
       " 'categories': 359,\n",
       " 'cavatelli': 402,\n",
       " 'central': 425,\n",
       " 'centre': 103,\n",
       " 'century': 181,\n",
       " 'cereals': 360,\n",
       " 'cheese': 426,\n",
       " 'china': 313,\n",
       " 'chutney': 147,\n",
       " 'clockwise': 74,\n",
       " 'come': 403,\n",
       " 'commercially': 388,\n",
       " 'common': 11,\n",
       " 'commonly': 361,\n",
       " 'compiled': 211,\n",
       " 'completed': 104,\n",
       " 'composition': 314,\n",
       " 'condiments': 427,\n",
       " 'conjecture': 182,\n",
       " 'consisting': 241,\n",
       " 'consumed': 148,\n",
       " 'consumption': 315,\n",
       " 'cook': 468,\n",
       " 'cooked': 316,\n",
       " 'cooking': 317,\n",
       " 'counter': 75,\n",
       " 'countries': 242,\n",
       " 'country': 183,\n",
       " 'court': 12,\n",
       " 'cricket': 105,\n",
       " 'crispier': 212,\n",
       " 'cuisine': 362,\n",
       " 'cultural': 318,\n",
       " 'cultures': 282,\n",
       " 'cut': 283,\n",
       " 'dating': 363,\n",
       " 'day': 184,\n",
       " 'de': 243,\n",
       " 'decorative': 404,\n",
       " 'deep': 319,\n",
       " 'demands': 492,\n",
       " 'depending': 405,\n",
       " 'derives': 320,\n",
       " 'designated': 106,\n",
       " 'diameter': 244,\n",
       " 'diet': 149,\n",
       " 'different': 406,\n",
       " 'discussing': 321,\n",
       " 'dish': 213,\n",
       " 'dishes': 364,\n",
       " 'dismissed': 107,\n",
       " 'divided': 365,\n",
       " 'documented': 407,\n",
       " 'dosa': 150,\n",
       " 'dosai': 185,\n",
       " 'dosaka': 214,\n",
       " 'doubles': 13,\n",
       " 'dough': 284,\n",
       " 'down': 245,\n",
       " 'dried': 322,\n",
       " 'during': 108,\n",
       " 'durum': 366,\n",
       " 'each': 63,\n",
       " 'eat': 285,\n",
       " 'eaten': 286,\n",
       " 'eggs': 367,\n",
       " 'either': 109,\n",
       " 'eleven': 110,\n",
       " 'encyclopedia': 215,\n",
       " 'end': 111,\n",
       " 'ends': 112,\n",
       " 'enduri': 270,\n",
       " 'europe': 469,\n",
       " 'european': 448,\n",
       " 'evidence': 323,\n",
       " 'exact': 186,\n",
       " 'example': 408,\n",
       " 'except': 493,\n",
       " 'exist': 470,\n",
       " 'extracted': 287,\n",
       " 'extras': 113,\n",
       " 'extruded': 288,\n",
       " 'extrusion': 389,\n",
       " 'fails': 494,\n",
       " 'far': 289,\n",
       " 'fast': 471,\n",
       " 'fermentation': 246,\n",
       " 'fermented': 151,\n",
       " 'field': 114,\n",
       " 'fielding': 64,\n",
       " 'filled': 409,\n",
       " 'first': 76,\n",
       " 'five': 247,\n",
       " 'flat': 290,\n",
       " 'flatbread': 428,\n",
       " 'flour': 368,\n",
       " 'folded': 324,\n",
       " 'follows': 495,\n",
       " 'food': 187,\n",
       " 'for': 216,\n",
       " 'form': 291,\n",
       " 'formal': 14,\n",
       " 'formed': 369,\n",
       " 'forms': 15,\n",
       " 'forth': 496,\n",
       " 'found': 217,\n",
       " 'founded': 449,\n",
       " 'four': 77,\n",
       " 'fresca': 370,\n",
       " 'fresh': 371,\n",
       " 'fried': 325,\n",
       " 'from': 152,\n",
       " 'frozen': 472,\n",
       " 'future': 326,\n",
       " 'gaeta': 429,\n",
       " 'gained': 115,\n",
       " 'game': 16,\n",
       " 'games': 17,\n",
       " 'generally': 430,\n",
       " 'geo': 327,\n",
       " 'german': 328,\n",
       " 'giving': 497,\n",
       " 'goans': 271,\n",
       " 'grains': 372,\n",
       " 'great': 498,\n",
       " 'guaranteed': 450,\n",
       " 'half': 18,\n",
       " 'hand': 390,\n",
       " 'hard': 499,\n",
       " 'has': 329,\n",
       " 'have': 116,\n",
       " 'having': 410,\n",
       " 'headquarters': 451,\n",
       " 'helices': 330,\n",
       " 'historian': 188,\n",
       " 'hit': 19,\n",
       " 'hitter': 500,\n",
       " 'hitting': 78,\n",
       " 'home': 79,\n",
       " 'hot': 153,\n",
       " 'households': 248,\n",
       " 'husked': 249,\n",
       " 'idli': 154,\n",
       " 'iii': 218,\n",
       " 'in': 20,\n",
       " 'inches': 250,\n",
       " 'include': 411,\n",
       " 'including': 117,\n",
       " 'india': 189,\n",
       " 'indian': 155,\n",
       " 'indigenous': 190,\n",
       " 'indoor': 21,\n",
       " 'ingredients': 156,\n",
       " 'initial': 501,\n",
       " 'innings': 118,\n",
       " 'into': 292,\n",
       " 'invented': 431,\n",
       " 'is': 22,\n",
       " 'it': 23,\n",
       " 'italian': 373,\n",
       " 'italy': 412,\n",
       " 'item': 473,\n",
       " 'its': 157,\n",
       " 'karnataka': 191,\n",
       " 'kind': 158,\n",
       " 'known': 272,\n",
       " 'konkani': 273,\n",
       " 'landing': 24,\n",
       " 'lanka': 251,\n",
       " 'large': 391,\n",
       " 'larger': 25,\n",
       " 'latin': 432,\n",
       " 'least': 502,\n",
       " 'lentils': 252,\n",
       " 'lightweight': 503,\n",
       " 'like': 253,\n",
       " 'limits': 504,\n",
       " 'linked': 219,\n",
       " 'literature': 192,\n",
       " 'locale': 413,\n",
       " 'long': 119,\n",
       " 'machines': 392,\n",
       " 'made': 159,\n",
       " 'main': 160,\n",
       " 'manasollasa': 220,\n",
       " 'manuscript': 433,\n",
       " 'many': 120,\n",
       " 'material': 331,\n",
       " 'matter': 193,\n",
       " 'may': 26,\n",
       " 'meats': 434,\n",
       " 'metabolised': 254,\n",
       " 'miniature': 414,\n",
       " 'mixed': 374,\n",
       " 'modern': 435,\n",
       " 'more': 255,\n",
       " 'most': 27,\n",
       " 'must': 332,\n",
       " 'nair': 194,\n",
       " 'names': 415,\n",
       " 'naples': 436,\n",
       " 'napoletana': 452,\n",
       " 'neapolitan': 453,\n",
       " 'neighbouring': 256,\n",
       " 'net': 28,\n",
       " 'nine': 65,\n",
       " 'non': 454,\n",
       " 'noodle': 293,\n",
       " 'noodles': 294,\n",
       " 'north': 474,\n",
       " 'nudel': 333,\n",
       " 'number': 121,\n",
       " 'odisha': 274,\n",
       " 'of': 29,\n",
       " 'often': 30,\n",
       " 'oil': 334,\n",
       " 'oldest': 335,\n",
       " 'on': 31,\n",
       " 'once': 295,\n",
       " 'one': 32,\n",
       " 'opponent': 505,\n",
       " 'opponents': 122,\n",
       " 'opposing': 33,\n",
       " 'opposite': 506,\n",
       " 'options': 507,\n",
       " 'or': 34,\n",
       " 'organisation': 455,\n",
       " 'origin': 221,\n",
       " 'original': 222,\n",
       " 'originated': 195,\n",
       " 'other': 275,\n",
       " 'outdoor': 35,\n",
       " 'oven': 437,\n",
       " 'ovens': 475,\n",
       " 'over': 161,\n",
       " 'overs': 123,\n",
       " 'paddle': 508,\n",
       " 'pan': 336,\n",
       " 'pancake': 162,\n",
       " 'parcel': 163,\n",
       " 'part': 164,\n",
       " 'pasta': 375,\n",
       " 'pastas': 376,\n",
       " 'people': 276,\n",
       " 'per': 36,\n",
       " 'period': 124,\n",
       " 'phase': 125,\n",
       " 'ping': 509,\n",
       " 'pitch': 126,\n",
       " 'pitcher': 80,\n",
       " 'pitha': 277,\n",
       " 'pizza': 438,\n",
       " 'place': 510,\n",
       " 'plate': 81,\n",
       " 'play': 127,\n",
       " 'played': 37,\n",
       " 'player': 38,\n",
       " 'players': 39,\n",
       " 'plural': 296,\n",
       " 'podi': 165,\n",
       " 'point': 511,\n",
       " 'points': 40,\n",
       " 'pong': 512,\n",
       " 'popular': 166,\n",
       " 'portions': 476,\n",
       " 'possible': 128,\n",
       " 'preparation': 477,\n",
       " 'prepared': 478,\n",
       " 'present': 196,\n",
       " 'probably': 223,\n",
       " 'process': 257,\n",
       " 'produced': 393,\n",
       " 'products': 394,\n",
       " 'profit': 456,\n",
       " 'promotes': 457,\n",
       " 'protects': 458,\n",
       " 'quick': 513,\n",
       " 'racquet': 41,\n",
       " 'racquets': 42,\n",
       " 'reactions': 514,\n",
       " 'readily': 258,\n",
       " 'recipe': 224,\n",
       " 'recorded': 439,\n",
       " 'rectangular': 43,\n",
       " 'refer': 377,\n",
       " 'reference': 378,\n",
       " 'references': 197,\n",
       " 'refrigerated': 337,\n",
       " 'region': 198,\n",
       " 'request': 459,\n",
       " 'restaurants': 225,\n",
       " 'return': 515,\n",
       " 'returns': 82,\n",
       " 'rice': 167,\n",
       " 'roles': 129,\n",
       " 'rolled': 297,\n",
       " 'ruled': 226,\n",
       " 'rules': 516,\n",
       " 'run': 83,\n",
       " 'running': 84,\n",
       " 'runs': 85,\n",
       " 'safeguarded': 460,\n",
       " 'salt': 338,\n",
       " 'sambar': 168,\n",
       " 'sangam': 199,\n",
       " 'sanna': 278,\n",
       " 'sanskrit': 227,\n",
       " 'sauce': 339,\n",
       " 'savoury': 259,\n",
       " 'scale': 395,\n",
       " 'score': 86,\n",
       " 'scored': 44,\n",
       " 'scores': 130,\n",
       " 'secca': 379,\n",
       " 'second': 87,\n",
       " 'see': 298,\n",
       " 'selection': 440,\n",
       " 'series': 88,\n",
       " 'serve': 299,\n",
       " 'served': 169,\n",
       " 'serving': 300,\n",
       " 'set': 131,\n",
       " 'several': 479,\n",
       " 'shapes': 301,\n",
       " 'sheets': 380,\n",
       " 'shells': 340,\n",
       " 'short': 341,\n",
       " 'shuttlecock': 45,\n",
       " 'sicily': 381,\n",
       " 'side': 46,\n",
       " 'similar': 480,\n",
       " 'simple': 396,\n",
       " 'since': 441,\n",
       " 'single': 302,\n",
       " 'singles': 47,\n",
       " 'sited': 132,\n",
       " 'small': 517,\n",
       " 'so': 260,\n",
       " 'softer': 228,\n",
       " 'sold': 481,\n",
       " 'someshvara': 229,\n",
       " 'sometimes': 342,\n",
       " 'soup': 343,\n",
       " 'south': 170,\n",
       " 'speciality': 461,\n",
       " 'specialty': 416,\n",
       " 'specific': 417,\n",
       " 'specified': 344,\n",
       " 'spinning': 518,\n",
       " 'sport': 48,\n",
       " 'sri': 261,\n",
       " 'st': 200,\n",
       " 'staple': 303,\n",
       " 'starches': 262,\n",
       " 'steaming': 263,\n",
       " 'storage': 345,\n",
       " 'stored': 346,\n",
       " 'stretched': 304,\n",
       " 'striking': 49,\n",
       " 'strings': 347,\n",
       " 'strips': 348,\n",
       " 'stromboli': 482,\n",
       " 'stuffed': 418,\n",
       " 'stumps': 133,\n",
       " 'subcontinent': 171,\n",
       " 'such': 483,\n",
       " 'supermarkets': 397,\n",
       " 'swap': 134,\n",
       " 'swung': 89,\n",
       " 'table': 519,\n",
       " 'take': 66,\n",
       " 'takes': 520,\n",
       " 'tamil': 201,\n",
       " 'team': 90,\n",
       " 'teams': 50,\n",
       " 'ten': 135,\n",
       " 'tennis': 521,\n",
       " 'term': 349,\n",
       " 'th': 230,\n",
       " 'thankappan': 202,\n",
       " 'that': 91,\n",
       " 'the': 51,\n",
       " 'their': 136,\n",
       " 'them': 484,\n",
       " 'then': 92,\n",
       " 'they': 264,\n",
       " 'thicker': 231,\n",
       " 'thin': 350,\n",
       " 'thinner': 232,\n",
       " 'third': 93,\n",
       " 'three': 137,\n",
       " 'throughout': 265,\n",
       " 'thrown': 94,\n",
       " 'thus': 305,\n",
       " 'time': 522,\n",
       " 'to': 52,\n",
       " 'today': 398,\n",
       " 'tomato': 442,\n",
       " 'topped': 443,\n",
       " 'toward': 523,\n",
       " 'town': 203,\n",
       " 'tradition': 233,\n",
       " 'traditional': 266,\n",
       " 'traditionally': 172,\n",
       " 'trajectory': 524,\n",
       " 'true': 462,\n",
       " 'tubes': 351,\n",
       " 'turns': 67,\n",
       " 'two': 53,\n",
       " 'types': 419,\n",
       " 'typically': 382,\n",
       " 'udupi': 204,\n",
       " 'union': 463,\n",
       " 'unleavened': 306,\n",
       " 'upon': 464,\n",
       " 'urad': 173,\n",
       " 'use': 205,\n",
       " 'used': 383,\n",
       " 'using': 54,\n",
       " 'usually': 267,\n",
       " 'variably': 420,\n",
       " 'variant': 279,\n",
       " 'variants': 444,\n",
       " 'varieties': 352,\n",
       " 'variety': 307,\n",
       " 'various': 384,\n",
       " 'vary': 421,\n",
       " 'vegetables': 445,\n",
       " 'verace': 465,\n",
       " 'version': 234,\n",
       " 'very': 280,\n",
       " 'via': 399,\n",
       " 'was': 206,\n",
       " 'water': 353,\n",
       " 'waves': 354,\n",
       " 'well': 174,\n",
       " 'wheat': 385,\n",
       " 'when': 95,\n",
       " 'which': 138,\n",
       " 'while': 355,\n",
       " 'whilst': 139,\n",
       " 'who': 68,\n",
       " 'whole': 485,\n",
       " 'wicket': 140,\n",
       " 'widely': 400,\n",
       " 'winning': 141,\n",
       " 'with': 55,\n",
       " 'within': 56,\n",
       " 'wooden': 142,\n",
       " 'word': 308,\n",
       " 'world': 446,\n",
       " 'yard': 57,\n",
       " 'years': 356}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a bag of words corpus in gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a bag of words corpus from a text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fhbapto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "[(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n",
      "[(5, 2), (12, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n",
      "[(3, 1), (9, 1), (12, 2), (18, 1), (22, 1), (26, 1), (32, 1), (33, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)]\n",
      "[(15, 1), (17, 1), (18, 1), (21, 1)]\n",
      "[(3, 1), (9, 1), (14, 1), (16, 1), (19, 1), (22, 2), (26, 2), (32, 1), (33, 1), (34, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]\n",
      "[(3, 1), (5, 2), (9, 1), (10, 1), (12, 1), (13, 1), (18, 1), (43, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1)]\n",
      "[(12, 1), (22, 1), (33, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1)]\n",
      "[(12, 3), (16, 1), (26, 1), (41, 1), (43, 1), (47, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1)]\n",
      "[(1, 1), (12, 1), (15, 1), (18, 1), (57, 1), (70, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1)]\n",
      "[(3, 1), (5, 1), (12, 2), (22, 1), (32, 1), (33, 1), (42, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to save a gensim dictionary and corpus to disk and load them back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Dict and Corpus\n",
    "mydict.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0)]\n",
      "[(14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0), (21, 1.0)]\n",
      "[(5, 2.0), (12, 1.0), (22, 2.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 1.0), (31, 1.0), (32, 1.0), (33, 1.0), (34, 1.0)]\n",
      "[(3, 1.0), (9, 1.0), (12, 2.0), (18, 1.0), (22, 1.0), (26, 1.0), (32, 1.0), (33, 1.0), (35, 1.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 1.0), (40, 1.0), (41, 1.0), (42, 1.0)]\n",
      "[(15, 1.0), (17, 1.0), (18, 1.0), (21, 1.0)]\n",
      "[(3, 1.0), (9, 1.0), (14, 1.0), (16, 1.0), (19, 1.0), (22, 2.0), (26, 2.0), (32, 1.0), (33, 1.0), (34, 1.0), (43, 1.0), (44, 1.0), (45, 1.0), (46, 1.0), (47, 1.0)]\n",
      "[(3, 1.0), (5, 2.0), (9, 1.0), (10, 1.0), (12, 1.0), (13, 1.0), (18, 1.0), (43, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (51, 1.0), (52, 1.0), (53, 1.0), (54, 1.0), (55, 1.0), (56, 1.0), (57, 1.0)]\n",
      "[(12, 1.0), (22, 1.0), (33, 1.0), (58, 1.0), (59, 1.0), (60, 1.0), (61, 1.0), (62, 1.0), (63, 1.0), (64, 1.0), (65, 1.0)]\n",
      "[(12, 3.0), (16, 1.0), (26, 1.0), (41, 1.0), (43, 1.0), (47, 1.0), (66, 1.0), (67, 1.0), (68, 1.0), (69, 1.0), (70, 1.0), (71, 1.0), (72, 1.0), (73, 1.0), (74, 1.0)]\n",
      "[(1, 1.0), (12, 1.0), (15, 1.0), (18, 1.0), (57, 1.0), (70, 1.0), (75, 1.0), (76, 1.0), (77, 1.0), (78, 2.0), (79, 1.0), (80, 1.0), (81, 1.0), (82, 1.0), (83, 1.0), (84, 1.0)]\n",
      "[(3, 1.0), (5, 1.0), (12, 2.0), (22, 1.0), (32, 1.0), (33, 1.0), (42, 1.0), (85, 1.0), (86, 1.0), (87, 1.0), (88, 1.0), (89, 1.0), (90, 1.0), (91, 1.0), (92, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
